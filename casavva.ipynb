{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3",
      "language": "python"
    },
    "language_info": {
      "name": "python",
      "version": "3.7.6",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "colab": {
      "name": "cassava_leaf_disease_training_with_tpu_v2_0de3c0_(2) (4).ipynb",
      "provenance": [],
      "collapsed_sections": [
        "mDLbByXTvUuU",
        "51gfJgtovUuW",
        "fcWo7-_tvUuY",
        "2lB2hfc6vUuk",
        "VxJdVC5kHZxK",
        "R7GdHIOPvUuw"
      ],
      "include_colab_link": true
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mandal-rahul/CS231n/blob/master/casavva.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8e3aBHbOvUuD"
      },
      "source": [
        "Improvements\n",
        " - `Custom training loop`: Using a custom training loop greatly improves the training time and resource usage.\n",
        " - `Maximize MXU and minimize Idle time`: I have made a few adjustments to the Tensorflow pipeline to improve performance.\n",
        "\n",
        "Experiments\n",
        " - Small improvements using external data (2019 competition).\n",
        " - Small improvements from using `CCE label smoothing`.\n",
        " - Small improvements from using `CutOut`.\n",
        " - Small improvements from `oversmapling` classes `0`, `1`, `2` and `4`.\n",
        " - Small improvements from keeping `batch normalization` layers frozen.\n",
        " - No relevant improvements from using `class weights`.\n",
        " - No relevant improvements from using `MixUp`.\n",
        " - No relevant improvements from using different backbones.\n",
        " - Worse performance by using different image resolution even the default `EfficientNet` input size.\n",
        " - Changing `Sparse CCE` to `CCE` has no impact, as expected.\n",
        " - Was not able to make progressive unfreezing work.\n",
        " - Changing the `learning rate` batch wise seems more efficent than epoch wise, specially for the warm up phase."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "papermill": {
          "duration": 0.033986,
          "end_time": "2020-10-10T21:25:09.858267",
          "exception": false,
          "start_time": "2020-10-10T21:25:09.824281",
          "status": "completed"
        },
        "tags": [],
        "id": "MT4skq6yvUuP"
      },
      "source": [
        "## Dependencies"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "_kg_hide-input": true,
        "trusted": true,
        "_kg_hide-output": true,
        "scrolled": false,
        "id": "7TDxp80uvUuR"
      },
      "source": [
        "colab = True\n",
        "if not colab:\n",
        "  !pip install -U --quiet tensorflow==2.3.2\n",
        "  !pip install  cloud-tpu-client\n",
        "!pip install image-classifiers\n",
        "!pip install  --upgrade  --quiet adabelief-tf\n",
        "!pip install --quiet --upgrade efficientnet"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2iNYxd3txhLl"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
        "_kg_hide-input": true,
        "_kg_hide-output": true,
        "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
        "execution": {
          "iopub.execute_input": "2020-10-10T21:25:09.94291Z",
          "iopub.status.busy": "2020-10-10T21:25:09.94211Z",
          "iopub.status.idle": "2020-10-10T21:25:26.556399Z",
          "shell.execute_reply": "2020-10-10T21:25:26.555607Z"
        },
        "papermill": {
          "duration": 16.665386,
          "end_time": "2020-10-10T21:25:26.556557",
          "exception": false,
          "start_time": "2020-10-10T21:25:09.891171",
          "status": "completed"
        },
        "tags": [],
        "trusted": true,
        "scrolled": true,
        "id": "VhuCdTQAvUuT"
      },
      "source": [
        "import math, os, re, warnings, random, time\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "from matplotlib import pyplot as plt\n",
        "from classification_models.keras import Classifiers\n",
        "if not colab:\n",
        "  from kaggle_datasets import KaggleDatasets\n",
        "from sklearn.model_selection import KFold\n",
        "from sklearn.metrics import classification_report, confusion_matrix,accuracy_score\n",
        "import tensorflow as tf\n",
        "import tensorflow.keras.layers as L\n",
        "import tensorflow.keras.backend as K\n",
        "from tensorflow.keras import optimizers, Sequential, losses, metrics, Model\n",
        "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping, ReduceLROnPlateau\n",
        "import efficientnet.tfkeras as efn\n",
        "import tensorflow_addons as tfa\n",
        "from adabelief_tf import AdaBeliefOptimizer\n",
        "from pathlib import Path\n",
        "from sklearn.metrics import roc_auc_score\n",
        "\n",
        "def seed_everything(seed=0):\n",
        "    random.seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    tf.random.set_seed(seed)\n",
        "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
        "    os.environ['TF_DETERMINISTIC_OPS'] = '1'\n",
        "\n",
        "seed = 0\n",
        "seed_everything(seed)\n",
        "warnings.filterwarnings('ignore')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "papermill": {
          "duration": 0.03302,
          "end_time": "2020-10-10T21:25:26.623975",
          "exception": false,
          "start_time": "2020-10-10T21:25:26.590955",
          "status": "completed"
        },
        "tags": [],
        "id": "mDLbByXTvUuU"
      },
      "source": [
        "### Hardware configuration\n",
        "\n",
        "Note that we have `32` cores, this is because the `TPU v2 Pod` have more cores than a single `TPU v3` which has `8` cores."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "_kg_hide-input": true,
        "execution": {
          "iopub.execute_input": "2020-10-10T21:25:26.705656Z",
          "iopub.status.busy": "2020-10-10T21:25:26.704813Z",
          "iopub.status.idle": "2020-10-10T21:25:31.882446Z",
          "shell.execute_reply": "2020-10-10T21:25:31.881768Z"
        },
        "papermill": {
          "duration": 5.225184,
          "end_time": "2020-10-10T21:25:31.882571",
          "exception": false,
          "start_time": "2020-10-10T21:25:26.657387",
          "status": "completed"
        },
        "tags": [],
        "trusted": true,
        "scrolled": true,
        "id": "JJCQ1bZwvUuV"
      },
      "source": [
        "# TPU or GPU detection\n",
        "# Detect hardware, return appropriate distribution strategy\n",
        "#from cloud_tpu_client import Client\n",
        "print(tf.__version__)\n",
        "\n",
        "\n",
        "try:\n",
        "    #Client().configure_tpu_version(tf.__version__, restart_type='ifNeeded')\n",
        "    tpu = tf.distribute.cluster_resolver.TPUClusterResolver()\n",
        "    print(f'Running on TPU {tpu.master()}')\n",
        "except ValueError:\n",
        "    tpu = None\n",
        "\n",
        "if tpu:\n",
        "    tf.config.experimental_connect_to_cluster(tpu)\n",
        "    tf.tpu.experimental.initialize_tpu_system(tpu)\n",
        "    strategy = tf.distribute.experimental.TPUStrategy(tpu)\n",
        "else:\n",
        "    strategy = tf.distribute.get_strategy()\n",
        "\n",
        "AUTO = tf.data.experimental.AUTOTUNE\n",
        "REPLICAS = strategy.num_replicas_in_sync\n",
        "print(f'REPLICAS: {REPLICAS}')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zTJBjA7pfJMw"
      },
      "source": [
        "# hyperparameters"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2020-10-10T21:25:32.028802Z",
          "iopub.status.busy": "2020-10-10T21:25:32.027887Z",
          "iopub.status.idle": "2020-10-10T21:25:32.031045Z",
          "shell.execute_reply": "2020-10-10T21:25:32.030416Z"
        },
        "papermill": {
          "duration": 0.044623,
          "end_time": "2020-10-10T21:25:32.031164",
          "exception": false,
          "start_time": "2020-10-10T21:25:31.986541",
          "status": "completed"
        },
        "tags": [],
        "trusted": true,
        "scrolled": true,
        "id": "LIiROenPvUuW"
      },
      "source": [
        "\n",
        "\n",
        "lr_schedule = 'cosine' \n",
        "          #'grad'\n",
        "BATCH_SIZE = 16 * REPLICAS*2\n",
        "T_1 = 0.4\n",
        "T_2 = 0.8\n",
        "SMOOTH_FRACTION = 0.2\n",
        "N_ITER = 5\n",
        "epsilon = 1e-16\n",
        "#lr start\n",
        "if lr_schedule == 'cosine':\n",
        "  LR_START = 1e-8\n",
        "  LR_MIN = 1e-8\n",
        "  LR_MAX = 0.00004 * strategy.num_replicas_in_sync\n",
        "  LR_RAMPUP_EPOCHS = 6\n",
        "  LR_SUSTAIN_EPOCHS = 0\n",
        "  N_CYCLES = .55\n",
        "else:    \n",
        "  LEARNING_RATE = 1e-5 * REPLICAS\n",
        "  LR_START = 0.00001\n",
        "  LR_MAX = 0.00004 * strategy.num_replicas_in_sync\n",
        "  LR_MIN = 0.00001\n",
        "  LR_RAMPUP_EPOCHS = 7\n",
        "  LR_SUSTAIN_EPOCHS = 0\n",
        "  LR_EXP_DECAY = .78\n",
        "# Lr end\n",
        "EPOCHS = 20\n",
        "HEIGHT = 512\n",
        "WIDTH = 512\n",
        "HEIGHT_RS = 512\n",
        "WIDTH_RS = 512\n",
        "CHANNELS = 3\n",
        "N_CLASSES = 5\n",
        "N_FOLDS = 3\n",
        "FOLDS_USED = 5\n",
        "patience = 10\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "class cfg:\n",
        "    \n",
        "    colab_instance=1\n",
        "    #base_save_path = '/content/drive/MyDrive/models/' \n",
        "    base_save_path = './models/'\n",
        "    if colab:\n",
        "      base_save_path = '/content/drive/MyDrive/models/'\n",
        "    models = [  efn.EfficientNetB2, \n",
        "                efn.EfficientNetB4, efn.EfficientNetB6\n",
        "             ]\n",
        "    model_input_shape = [512,512,512]\n",
        "    #img_resize_shape =512\n",
        "    model = efn.EfficientNetB4\n",
        "    model_name = model.__name__\n",
        "    num_exp = 5                      #     models list should be in asc order of input size\n",
        "    display_dataset = True\n",
        "    run_exp = True, \n",
        "    exp_csv_path =base_save_path+f'Accuracy_{colab_instance}.csv'\n",
        "    optimizers = ['adam','ranger','adabelief']\n",
        "    optimiser_name = 'ranger'\n",
        "    external_data = False\n",
        "    \n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "papermill": {
          "duration": 0.034123,
          "end_time": "2020-10-10T21:25:31.952284",
          "exception": false,
          "start_time": "2020-10-10T21:25:31.918161",
          "status": "completed"
        },
        "tags": [],
        "id": "3BQhV3MzvUuV"
      },
      "source": [
        "# Learning rate scheduler"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "_kg_hide-input": true,
        "execution": {
          "iopub.execute_input": "2020-10-10T21:30:00.81171Z",
          "iopub.status.busy": "2020-10-10T21:30:00.803307Z",
          "iopub.status.idle": "2020-10-10T21:30:01.170422Z",
          "shell.execute_reply": "2020-10-10T21:30:01.169829Z"
        },
        "papermill": {
          "duration": 0.532334,
          "end_time": "2020-10-10T21:30:01.170556",
          "exception": false,
          "start_time": "2020-10-10T21:30:00.638222",
          "status": "completed"
        },
        "tags": [],
        "trusted": true,
        "scrolled": true,
        "id": "u4blyyFvvUux"
      },
      "source": [
        "def lrfn(epoch):\n",
        "  if lr_schedule == 'cosine':\n",
        "    return lrfnCosine(epoch)\n",
        "  else:\n",
        "    return lrfnGrad(epoch)\n",
        "\n",
        "def lrfnCosine(epoch):\n",
        "    if epoch < LR_RAMPUP_EPOCHS:\n",
        "        lr = (LR_MAX - LR_START) / LR_RAMPUP_EPOCHS * epoch + LR_START\n",
        "    elif epoch < LR_RAMPUP_EPOCHS + LR_SUSTAIN_EPOCHS:\n",
        "        lr = LR_MAX\n",
        "    else:\n",
        "        progress = (epoch - LR_RAMPUP_EPOCHS - LR_SUSTAIN_EPOCHS) / (EPOCHS - LR_RAMPUP_EPOCHS - LR_SUSTAIN_EPOCHS)\n",
        "        lr = LR_MAX * (0.5 * (1.0 + tf.math.cos(math.pi * N_CYCLES * 2.0 * progress)))\n",
        "        if LR_MIN is not None:\n",
        "            lr = tf.math.maximum(LR_MIN, lr)\n",
        "            \n",
        "    return lr\n",
        "\n",
        "def lrfnGrad(epoch):\n",
        "    if epoch < LR_RAMPUP_EPOCHS:\n",
        "        lr = (LR_MAX - LR_START) / LR_RAMPUP_EPOCHS * epoch + LR_START\n",
        "    elif epoch < LR_RAMPUP_EPOCHS + LR_SUSTAIN_EPOCHS:\n",
        "        lr = LR_MAX\n",
        "    else:\n",
        "        lr = (LR_MAX - LR_MIN) * LR_EXP_DECAY**(epoch - LR_RAMPUP_EPOCHS - LR_SUSTAIN_EPOCHS) \n",
        "    return lr\n",
        "\n",
        "rng = [i for i in range(EPOCHS)]\n",
        "y = [lrfn(x) for x in rng]\n",
        "plt.plot(rng, y)\n",
        "print(\"Learning rate schedule: {:.3g} to {:.3g} to {:.3g}\".format(y[0], max(y), y[-1]))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "51gfJgtovUuW"
      },
      "source": [
        "# Load data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2020-10-10T21:25:32.118263Z",
          "iopub.status.busy": "2020-10-10T21:25:32.115381Z",
          "iopub.status.idle": "2020-10-10T21:25:32.677665Z",
          "shell.execute_reply": "2020-10-10T21:25:32.677043Z"
        },
        "papermill": {
          "duration": 0.61129,
          "end_time": "2020-10-10T21:25:32.677805",
          "exception": false,
          "start_time": "2020-10-10T21:25:32.066515",
          "status": "completed"
        },
        "tags": [],
        "_kg_hide-input": true,
        "trusted": true,
        "scrolled": true,
        "id": "z3nxaP4svUuX"
      },
      "source": [
        "def count_data_items(filenames):\n",
        "    n = [int(re.compile(r'-([0-9]*)\\.').search(filename).group(1)) for filename in filenames]\n",
        "    return np.sum(n)\n",
        "\n",
        "\n",
        "database_base_path = '/kaggle/input/cassava-leaf-disease-classification/'\n",
        "#train = pd.read_csv(f'{database_base_path}train.csv')\n",
        "#print(f'Train samples: {len(train)}')\n",
        "if colab:\n",
        "  GCS_PATH = 'gs://kds-579a462f598557fb5d480ef78228eeb1d1ea5ce43dc47e674fa5a644'\n",
        "  GCS_PATH_EXT = 'gs://kds-388cbf339ce4bb04f6032d8af6dfeab8858192d193051d8be6df0631' \n",
        "  GCS_PATH_CLASSES = 'gs://kds-ebf0f9ac962adcdab49275d8fb50516ecedc4b726a0944f99b4ff60f'\n",
        "  GCS_PATH_EXT_CLASSES = 'gs://kds-62aaac1dd1d2587cfc0981ec461352d1527965ab1e895d7d253a8816'  \n",
        "else:\n",
        "# GCS_PATH = KaggleDatasets().get_gcs_path('cassava-leaf-disease-classification') # Original dataset\n",
        "  GCS_PATH = KaggleDatasets().get_gcs_path(f'cassava-leaf-disease-50-tfrecords-center-512x512') # Center croped and resized (50 TFRecord)\n",
        "  GCS_PATH_EXT = KaggleDatasets().get_gcs_path(f'cassava-leaf-disease-50-tfrecords-external-512x512') # Center croped and resized (50 TFRecord) (External)\n",
        "  GCS_PATH_CLASSES = KaggleDatasets().get_gcs_path(f'cassava-leaf-disease-50-tfrecords-classes-512x512') # Center croped and resized (50 TFRecord) by classes\n",
        "  GCS_PATH_EXT_CLASSES = KaggleDatasets().get_gcs_path(f'cassava-leaf-disease-ext-50-tfrec-classes-512x512') # Center croped and resized (50 TFRecord) (External) by classes\n",
        "\n",
        "\n",
        "\n",
        "# FILENAMES_COMP = tf.io.gfile.glob(GCS_PATH + '/train_tfrecords/*.tfrec') # Original TFRecords\n",
        "FILENAMES_COMP = tf.io.gfile.glob(GCS_PATH + '/*.tfrec')\n",
        "FILENAMES_2019 = tf.io.gfile.glob(GCS_PATH_EXT + '/*.tfrec')\n",
        "\n",
        "FILENAMES_COMP_CBB = tf.io.gfile.glob(GCS_PATH_CLASSES + '/CBB*.tfrec')\n",
        "FILENAMES_COMP_CBSD = tf.io.gfile.glob(GCS_PATH_CLASSES + '/CBSD*.tfrec')\n",
        "FILENAMES_COMP_CGM = tf.io.gfile.glob(GCS_PATH_CLASSES + '/CGM*.tfrec')\n",
        "FILENAMES_COMP_CMD = tf.io.gfile.glob(GCS_PATH_CLASSES + '/CMD*.tfrec')\n",
        "FILENAMES_COMP_Healthy = tf.io.gfile.glob(GCS_PATH_CLASSES + '/Healthy*.tfrec')\n",
        "\n",
        "FILENAMES_2019_CBB = tf.io.gfile.glob(GCS_PATH_EXT_CLASSES + '/CBB*.tfrec')\n",
        "FILENAMES_2019_CBSD = tf.io.gfile.glob(GCS_PATH_EXT_CLASSES + '/CBSD*.tfrec')\n",
        "FILENAMES_2019_CGM = tf.io.gfile.glob(GCS_PATH_EXT_CLASSES + '/CGM*.tfrec')\n",
        "FILENAMES_2019_CMD = tf.io.gfile.glob(GCS_PATH_EXT_CLASSES + '/CMD*.tfrec')\n",
        "FILENAMES_2019_Healthy = tf.io.gfile.glob(GCS_PATH_EXT_CLASSES + '/Healthy*.tfrec')\n",
        "\n",
        "\n",
        "TRAINING_FILENAMES = (FILENAMES_COMP + \n",
        "                      FILENAMES_2019 + \n",
        "                      (2 * FILENAMES_COMP_CBB) + \n",
        "                      (2 * FILENAMES_2019_CBB) + \n",
        "                      (2 * FILENAMES_COMP_CBSD) + \n",
        "                      (2 * FILENAMES_2019_CBSD) + \n",
        "                      (2 * FILENAMES_COMP_CGM) + \n",
        "                      (2 * FILENAMES_2019_CGM) + \n",
        "                      (2 * FILENAMES_COMP_Healthy) + \n",
        "                      (2 * FILENAMES_2019_Healthy))\n",
        "\n",
        "NUM_TRAINING_IMAGES = count_data_items(TRAINING_FILENAMES)\n",
        "\n",
        "print(f'GCS: train images: {NUM_TRAINING_IMAGES}')\n",
        "#display(train.head())\n",
        "\n",
        "CLASSES = ['Cassava Bacterial Blight', \n",
        "           'Cassava Brown Streak Disease', \n",
        "           'Cassava Green Mottle', \n",
        "           'Cassava Mosaic Disease', \n",
        "           'Healthy']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fcWo7-_tvUuY"
      },
      "source": [
        "# Augmentation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "scrolled": true,
        "id": "_VDe15p5vUua"
      },
      "source": [
        "def data_augment(image, label):\n",
        "    p_rotation = tf.random.uniform([], 0, 1.0, dtype=tf.float32)\n",
        "    p_spatial = tf.random.uniform([], 0, 1.0, dtype=tf.float32)\n",
        "    p_rotate = tf.random.uniform([], 0, 1.0, dtype=tf.float32)\n",
        "    p_pixel_1 = tf.random.uniform([], 0, 1.0, dtype=tf.float32)\n",
        "    p_pixel_2 = tf.random.uniform([], 0, 1.0, dtype=tf.float32)\n",
        "    p_pixel_3 = tf.random.uniform([], 0, 1.0, dtype=tf.float32)\n",
        "    p_shear = tf.random.uniform([], 0, 1.0, dtype=tf.float32)\n",
        "    p_crop = tf.random.uniform([], 0, 1.0, dtype=tf.float32)\n",
        "    p_cutout = tf.random.uniform([], 0, 1.0, dtype=tf.float32)\n",
        "    \n",
        "    # Shear\n",
        "    if p_shear > .2:\n",
        "        if p_shear > .6:\n",
        "            image = transform_shear(image, HEIGHT, shear=20.)\n",
        "        else:\n",
        "            image = transform_shear(image, HEIGHT, shear=-20.)\n",
        "            \n",
        "    # Rotation\n",
        "    if p_rotation > .2:\n",
        "        if p_rotation > .6:\n",
        "            image = transform_rotation(image, HEIGHT, rotation=45.)\n",
        "        else:\n",
        "            image = transform_rotation(image, HEIGHT, rotation=-45.)\n",
        "            \n",
        "    # Flips\n",
        "    image = tf.image.random_flip_left_right(image)\n",
        "    image = tf.image.random_flip_up_down(image)\n",
        "    if p_spatial > .75:\n",
        "        image = tf.image.transpose(image)\n",
        "        \n",
        "    # Rotates\n",
        "    if p_rotate > .75:\n",
        "        image = tf.image.rot90(image, k=3) # rotate 270ยบ\n",
        "    elif p_rotate > .5:\n",
        "        image = tf.image.rot90(image, k=2) # rotate 180ยบ\n",
        "    elif p_rotate > .25:\n",
        "        image = tf.image.rot90(image, k=1) # rotate 90ยบ\n",
        "        \n",
        "    # Pixel-level transforms\n",
        "    if p_pixel_1 >= .4:\n",
        "        image = tf.image.random_saturation(image, lower=.7, upper=1.3)\n",
        "    if p_pixel_2 >= .4:\n",
        "        image = tf.image.random_contrast(image, lower=.8, upper=1.2)\n",
        "    if p_pixel_3 >= .4:\n",
        "        image = tf.image.random_brightness(image, max_delta=.1)\n",
        "        \n",
        "    # Crops\n",
        "    if p_crop > .6:\n",
        "        if p_crop > .9:\n",
        "            image = tf.image.central_crop(image, central_fraction=.5)\n",
        "        elif p_crop > .8:\n",
        "            image = tf.image.central_crop(image, central_fraction=.6)\n",
        "        elif p_crop > .7:\n",
        "            image = tf.image.central_crop(image, central_fraction=.7)\n",
        "        else:\n",
        "            image = tf.image.central_crop(image, central_fraction=.8)\n",
        "    elif p_crop > .3:\n",
        "        crop_size = tf.random.uniform([], int(HEIGHT*.6), HEIGHT, dtype=tf.int32)\n",
        "        image = tf.image.random_crop(image, size=[crop_size, crop_size, CHANNELS])\n",
        "            \n",
        "    image = tf.image.resize(image, size=[HEIGHT, WIDTH])\n",
        "\n",
        "    if p_cutout > .5:\n",
        "        image = data_augment_cutout(image)\n",
        "        \n",
        "    return image, label"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NUBpxHTIvUuc"
      },
      "source": [
        "## Auxiliary functions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "_kg_hide-input": true,
        "trusted": true,
        "scrolled": true,
        "id": "HL_6kQZTvUud"
      },
      "source": [
        "# data augmentation @cdeotte kernel: https://www.kaggle.com/cdeotte/rotation-augmentation-gpu-tpu-0-96\n",
        "def transform_rotation(image, height, rotation):\n",
        "    # input image - is one image of size [dim,dim,3] not a batch of [b,dim,dim,3]\n",
        "    # output - image randomly rotated\n",
        "    DIM = height\n",
        "    XDIM = DIM%2 #fix for size 331\n",
        "    \n",
        "    rotation = rotation * tf.random.uniform([1],dtype='float32')\n",
        "    # CONVERT DEGREES TO RADIANS\n",
        "    rotation = math.pi * rotation / 180.\n",
        "    \n",
        "    # ROTATION MATRIX\n",
        "    c1 = tf.math.cos(rotation)\n",
        "    s1 = tf.math.sin(rotation)\n",
        "    one = tf.constant([1],dtype='float32')\n",
        "    zero = tf.constant([0],dtype='float32')\n",
        "    rotation_matrix = tf.reshape(tf.concat([c1,s1,zero, -s1,c1,zero, zero,zero,one],axis=0),[3,3])\n",
        "\n",
        "    # LIST DESTINATION PIXEL INDICES\n",
        "    x = tf.repeat( tf.range(DIM//2,-DIM//2,-1), DIM )\n",
        "    y = tf.tile( tf.range(-DIM//2,DIM//2),[DIM] )\n",
        "    z = tf.ones([DIM*DIM],dtype='int32')\n",
        "    idx = tf.stack( [x,y,z] )\n",
        "    \n",
        "    # ROTATE DESTINATION PIXELS ONTO ORIGIN PIXELS\n",
        "    idx2 = K.dot(rotation_matrix,tf.cast(idx,dtype='float32'))\n",
        "    idx2 = K.cast(idx2,dtype='int32')\n",
        "    idx2 = K.clip(idx2,-DIM//2+XDIM+1,DIM//2)\n",
        "    \n",
        "    # FIND ORIGIN PIXEL VALUES \n",
        "    idx3 = tf.stack( [DIM//2-idx2[0,], DIM//2-1+idx2[1,]] )\n",
        "    d = tf.gather_nd(image, tf.transpose(idx3))\n",
        "        \n",
        "    return tf.reshape(d,[DIM,DIM,3])\n",
        "\n",
        "def transform_shear(image, height, shear):\n",
        "    # input image - is one image of size [dim,dim,3] not a batch of [b,dim,dim,3]\n",
        "    # output - image randomly sheared\n",
        "    DIM = height\n",
        "    XDIM = DIM%2 #fix for size 331\n",
        "    \n",
        "    shear = shear * tf.random.uniform([1],dtype='float32')\n",
        "    shear = math.pi * shear / 180.\n",
        "        \n",
        "    # SHEAR MATRIX\n",
        "    one = tf.constant([1],dtype='float32')\n",
        "    zero = tf.constant([0],dtype='float32')\n",
        "    c2 = tf.math.cos(shear)\n",
        "    s2 = tf.math.sin(shear)\n",
        "    shear_matrix = tf.reshape(tf.concat([one,s2,zero, zero,c2,zero, zero,zero,one],axis=0),[3,3])    \n",
        "\n",
        "    # LIST DESTINATION PIXEL INDICES\n",
        "    x = tf.repeat( tf.range(DIM//2,-DIM//2,-1), DIM )\n",
        "    y = tf.tile( tf.range(-DIM//2,DIM//2),[DIM] )\n",
        "    z = tf.ones([DIM*DIM],dtype='int32')\n",
        "    idx = tf.stack( [x,y,z] )\n",
        "    \n",
        "    # ROTATE DESTINATION PIXELS ONTO ORIGIN PIXELS\n",
        "    idx2 = K.dot(shear_matrix,tf.cast(idx,dtype='float32'))\n",
        "    idx2 = K.cast(idx2,dtype='int32')\n",
        "    idx2 = K.clip(idx2,-DIM//2+XDIM+1,DIM//2)\n",
        "    \n",
        "    # FIND ORIGIN PIXEL VALUES \n",
        "    idx3 = tf.stack( [DIM//2-idx2[0,], DIM//2-1+idx2[1,]] )\n",
        "    d = tf.gather_nd(image, tf.transpose(idx3))\n",
        "        \n",
        "    return tf.reshape(d,[DIM,DIM,3])\n",
        "\n",
        "# CutOut\n",
        "def data_augment_cutout(image, min_mask_size=(int(HEIGHT * .1), int(HEIGHT * .1)), \n",
        "                        max_mask_size=(int(HEIGHT * .125), int(HEIGHT * .125))):\n",
        "    p_cutout = tf.random.uniform([], 0, 1.0, dtype=tf.float32)\n",
        "    \n",
        "    if p_cutout > .85: # 10~15 cut outs\n",
        "        n_cutout = tf.random.uniform([], 10, 15, dtype=tf.int32)\n",
        "        image = random_cutout(image, HEIGHT, WIDTH, \n",
        "                              min_mask_size=min_mask_size, max_mask_size=max_mask_size, k=n_cutout)\n",
        "    elif p_cutout > .6: # 5~10 cut outs\n",
        "        n_cutout = tf.random.uniform([], 5, 10, dtype=tf.int32)\n",
        "        image = random_cutout(image, HEIGHT, WIDTH, \n",
        "                              min_mask_size=min_mask_size, max_mask_size=max_mask_size, k=n_cutout)\n",
        "    elif p_cutout > .25: # 2~5 cut outs\n",
        "        n_cutout = tf.random.uniform([], 2, 5, dtype=tf.int32)\n",
        "        image = random_cutout(image, HEIGHT, WIDTH, \n",
        "                              min_mask_size=min_mask_size, max_mask_size=max_mask_size, k=n_cutout)\n",
        "    else: # 1 cut out\n",
        "        image = random_cutout(image, HEIGHT, WIDTH, \n",
        "                              min_mask_size=min_mask_size, max_mask_size=max_mask_size, k=1)\n",
        "\n",
        "    return image\n",
        "\n",
        "def random_cutout(image, height, width, channels=3, min_mask_size=(10, 10), max_mask_size=(80, 80), k=1):\n",
        "    assert height > min_mask_size[0]\n",
        "    assert width > min_mask_size[1]\n",
        "    assert height > max_mask_size[0]\n",
        "    assert width > max_mask_size[1]\n",
        "\n",
        "    for i in range(k):\n",
        "      mask_height = tf.random.uniform(shape=[], minval=min_mask_size[0], maxval=max_mask_size[0], dtype=tf.int32)\n",
        "      mask_width = tf.random.uniform(shape=[], minval=min_mask_size[1], maxval=max_mask_size[1], dtype=tf.int32)\n",
        "\n",
        "      pad_h = height - mask_height\n",
        "      pad_top = tf.random.uniform(shape=[], minval=0, maxval=pad_h, dtype=tf.int32)\n",
        "      pad_bottom = pad_h - pad_top\n",
        "\n",
        "      pad_w = width - mask_width\n",
        "      pad_left = tf.random.uniform(shape=[], minval=0, maxval=pad_w, dtype=tf.int32)\n",
        "      pad_right = pad_w - pad_left\n",
        "\n",
        "      cutout_area = tf.zeros(shape=[mask_height, mask_width, channels], dtype=tf.uint8)\n",
        "\n",
        "      cutout_mask = tf.pad([cutout_area], [[0,0],[pad_top, pad_bottom], [pad_left, pad_right], [0,0]], constant_values=1)\n",
        "      cutout_mask = tf.squeeze(cutout_mask, axis=0)\n",
        "      image = tf.multiply(tf.cast(image, tf.float32), tf.cast(cutout_mask, tf.float32))\n",
        "\n",
        "    return image"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
        "_kg_hide-input": true,
        "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a",
        "execution": {
          "iopub.execute_input": "2020-10-10T21:25:32.781506Z",
          "iopub.status.busy": "2020-10-10T21:25:32.777062Z",
          "iopub.status.idle": "2020-10-10T21:25:32.784982Z",
          "shell.execute_reply": "2020-10-10T21:25:32.78432Z"
        },
        "papermill": {
          "duration": 0.072304,
          "end_time": "2020-10-10T21:25:32.785102",
          "exception": false,
          "start_time": "2020-10-10T21:25:32.712798",
          "status": "completed"
        },
        "tags": [],
        "trusted": true,
        "scrolled": true,
        "id": "dWGYgfcDvUuh"
      },
      "source": [
        "# Datasets utility functions\n",
        "def decode_image(image_data):\n",
        "    \"\"\"\n",
        "        Decode a JPEG-encoded image to a uint8 tensor.\n",
        "    \"\"\"\n",
        "    image = tf.image.decode_jpeg(image_data, channels=3)\n",
        "    return image\n",
        "\n",
        "def scale_image(image, label):\n",
        "    \"\"\"\n",
        "        Cast tensor to float and normalizes (range between 0 and 1).\n",
        "    \"\"\"\n",
        "    image = tf.cast(image, tf.float32)\n",
        "    image /= 255.0\n",
        "    return image, label\n",
        "\n",
        "def prepare_image(image, label):\n",
        "    \"\"\"\n",
        "        Resize and reshape images to the expected size.\n",
        "    \"\"\"\n",
        "    image = tf.image.resize(image, [HEIGHT_RS, WIDTH_RS])\n",
        "    image = tf.reshape(image, [HEIGHT_RS, WIDTH_RS, 3])\n",
        "    return image, label\n",
        "\n",
        "def read_tfrecord(example, labeled=True):\n",
        "    \"\"\"\n",
        "        1. Parse data based on the 'TFREC_FORMAT' map.\n",
        "        2. Decode image.\n",
        "        3. If 'labeled' returns (image, label) if not (image, name).\n",
        "    \"\"\"\n",
        "    if labeled:\n",
        "        TFREC_FORMAT = {\n",
        "            'image': tf.io.FixedLenFeature([], tf.string), \n",
        "            'target': tf.io.FixedLenFeature([], tf.int64), \n",
        "        }\n",
        "    else:\n",
        "        TFREC_FORMAT = {\n",
        "            'image': tf.io.FixedLenFeature([], tf.string), \n",
        "            'image_name': tf.io.FixedLenFeature([], tf.string), \n",
        "        }\n",
        "    example = tf.io.parse_single_example(example, TFREC_FORMAT)\n",
        "    image = decode_image(example['image'])\n",
        "    if labeled:\n",
        "        label_or_name = tf.cast(example['target'], tf.int32)\n",
        "        # One-Hot Encoding needed to use \"categorical_crossentropy\" loss\n",
        "        label_or_name = tf.one_hot(tf.cast(label_or_name, tf.int32), N_CLASSES)\n",
        "    else:\n",
        "        label_or_name = example['image_name']\n",
        "    return image, label_or_name\n",
        "\n",
        "def get_dataset(FILENAMES, labeled=True, ordered=False, repeated=False, \n",
        "                cached=False, augment=False):\n",
        "    \"\"\"\n",
        "        Return a Tensorflow dataset ready for training or inference.\n",
        "    \"\"\"\n",
        "    ignore_order = tf.data.Options()\n",
        "    if not ordered:\n",
        "        ignore_order.experimental_deterministic = False\n",
        "        dataset = tf.data.Dataset.list_files(FILENAMES)\n",
        "        dataset = dataset.interleave(tf.data.TFRecordDataset, num_parallel_calls=AUTO)\n",
        "    else:\n",
        "        dataset = tf.data.TFRecordDataset(FILENAMES, num_parallel_reads=AUTO)\n",
        "        \n",
        "    dataset = dataset.with_options(ignore_order)\n",
        "    \n",
        "    dataset = dataset.map(lambda x: read_tfrecord(x, labeled=labeled), num_parallel_calls=AUTO)\n",
        "    \n",
        "    if augment:\n",
        "        dataset = dataset.map(data_augment, num_parallel_calls=AUTO)\n",
        "        \n",
        "    dataset = dataset.map(scale_image, num_parallel_calls=AUTO)\n",
        "    dataset = dataset.map(prepare_image, num_parallel_calls=AUTO)\n",
        "    \n",
        "    if not ordered:\n",
        "        dataset = dataset.shuffle(2048)\n",
        "    if repeated:\n",
        "        dataset = dataset.repeat()\n",
        "        \n",
        "    dataset = dataset.batch(BATCH_SIZE)\n",
        "    \n",
        "    if cached:\n",
        "        dataset = dataset.cache()\n",
        "    dataset = dataset.prefetch(AUTO)\n",
        "    return dataset\n",
        "\n",
        "def unfreeze_model(model):\n",
        "    # Unfreeze layers while leaving BatchNorm layers frozen\n",
        "    for layer in model.layers:\n",
        "        if not isinstance(layer, L.BatchNormalization):\n",
        "            layer.trainable = True\n",
        "        else:\n",
        "            layer.trainable = False\n",
        "                \n",
        "def unfreeze_block(model, block_name=None, n_top=3):\n",
        "    # Unfreeze layers while leaving BatchNorm layers frozen\n",
        "    for layer in model.layers[:-n_top]:\n",
        "        if isinstance(layer, L.BatchNormalization):\n",
        "            layer.trainable = False\n",
        "        else:\n",
        "            if block_name and (block_name in layer.name):\n",
        "                layer.trainable = True"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "_kg_hide-input": true,
        "execution": {
          "iopub.execute_input": "2020-10-10T21:25:32.896825Z",
          "iopub.status.busy": "2020-10-10T21:25:32.886455Z",
          "iopub.status.idle": "2020-10-10T21:25:32.900548Z",
          "shell.execute_reply": "2020-10-10T21:25:32.899792Z"
        },
        "papermill": {
          "duration": 0.08061,
          "end_time": "2020-10-10T21:25:32.900668",
          "exception": false,
          "start_time": "2020-10-10T21:25:32.820058",
          "status": "completed"
        },
        "tags": [],
        "trusted": true,
        "scrolled": true,
        "id": "sydna5n9vUuj"
      },
      "source": [
        "# Visualization utility functions\n",
        "np.set_printoptions(threshold=15, linewidth=80)\n",
        "\n",
        "def batch_to_numpy_images_and_labels(data):\n",
        "    images, labels = data\n",
        "    numpy_images = images.numpy()\n",
        "    numpy_labels = labels.numpy()\n",
        "    if numpy_labels.dtype == object: # binary string in this case, these are image ID strings\n",
        "        numpy_labels = [None for _ in enumerate(numpy_images)]\n",
        "    # If no labels, only image IDs, return None for labels (this is the case for test data)\n",
        "    return numpy_images, numpy_labels\n",
        "\n",
        "def title_from_label_and_target(label, correct_label):\n",
        "    if correct_label is None:\n",
        "        return CLASSES[label], True\n",
        "    correct = (label == correct_label)\n",
        "    return \"{} [{}{}{}]\".format(CLASSES[label], 'OK' if correct else 'NO', u\"\\u2192\" if not correct else '',\n",
        "                                CLASSES[correct_label] if not correct else ''), correct\n",
        "\n",
        "def display_one_flower(image, title, subplot, red=False, titlesize=16):\n",
        "    plt.subplot(*subplot)\n",
        "    plt.axis('off')\n",
        "    plt.imshow(image)\n",
        "    if len(title) > 0:\n",
        "        plt.title(title, fontsize=int(titlesize) if not red else int(titlesize/1.2), color='red' if red else 'black', \n",
        "                  fontdict={'verticalalignment':'center'}, pad=int(titlesize/1.5))\n",
        "    return (subplot[0], subplot[1], subplot[2]+1)\n",
        "\n",
        "def display_batch_of_images(databatch, predictions=None):\n",
        "    \"\"\"This will work with:\n",
        "    display_batch_of_images(images)\n",
        "    display_batch_of_images(images, predictions)\n",
        "    display_batch_of_images((images, labels))\n",
        "    display_batch_of_images((images, labels), predictions)\n",
        "    \"\"\"\n",
        "    # data\n",
        "    images, labels = batch_to_numpy_images_and_labels(databatch)\n",
        "    labels = np.argmax(labels, axis=-1)\n",
        "    if labels is None:\n",
        "        labels = [None for _ in enumerate(images)]\n",
        "        \n",
        "    # auto-squaring: this will drop data that does not fit into square or square-ish rectangle\n",
        "    rows = int(math.sqrt(len(images)))\n",
        "    cols = len(images)//rows\n",
        "        \n",
        "    # size and spacing\n",
        "    FIGSIZE = 13.0\n",
        "    SPACING = 0.1\n",
        "    subplot=(rows,cols,1)\n",
        "    if rows < cols:\n",
        "        plt.figure(figsize=(FIGSIZE,FIGSIZE/cols*rows))\n",
        "    else:\n",
        "        plt.figure(figsize=(FIGSIZE/rows*cols,FIGSIZE))\n",
        "    \n",
        "    # display\n",
        "    for i, (image, label) in enumerate(zip(images[:rows*cols], labels[:rows*cols])):\n",
        "        title = '' if label is None else CLASSES[label]\n",
        "        correct = True\n",
        "        if predictions is not None:\n",
        "            title, correct = title_from_label_and_target(predictions[i], label)\n",
        "        dynamic_titlesize = FIGSIZE*SPACING/max(rows,cols)*40+3 # magic formula tested to work from 1x1 to 10x10 images\n",
        "        subplot = display_one_flower(image, title, subplot, not correct, titlesize=dynamic_titlesize)\n",
        "    \n",
        "    #layout\n",
        "    plt.tight_layout()\n",
        "    if label is None and predictions is None:\n",
        "        plt.subplots_adjust(wspace=0, hspace=0)\n",
        "    else:\n",
        "        plt.subplots_adjust(wspace=SPACING, hspace=SPACING)\n",
        "    plt.show()\n",
        "    \n",
        "# Visualize model predictions\n",
        "def dataset_to_numpy_util(dataset, N):\n",
        "    dataset = dataset.unbatch().batch(N)\n",
        "    for images, labels in dataset:\n",
        "        numpy_images = images.numpy()\n",
        "        numpy_labels = labels.numpy()\n",
        "        break;  \n",
        "    return numpy_images, numpy_labels\n",
        "\n",
        "def title_from_label_and_target(label, correct_label):\n",
        "    label = np.argmax(label, axis=-1)\n",
        "    correct = (label == correct_label)\n",
        "    return \"{} [{}{}{}]\".format(label, str(correct), ', shoud be ' if not correct else '',\n",
        "                                correct_label if not correct else ''), correct\n",
        "\n",
        "def display_one_flower_eval(image, title, subplot, red=False):\n",
        "    plt.subplot(subplot)\n",
        "    plt.axis('off')\n",
        "    plt.imshow(image)\n",
        "    plt.title(title, fontsize=14, color='red' if red else 'black')\n",
        "    return subplot+1\n",
        "\n",
        "def display_9_images_with_predictions(images, predictions, labels):\n",
        "    subplot=331\n",
        "    plt.figure(figsize=(13,13))\n",
        "    for i, image in enumerate(images):\n",
        "        title, correct = title_from_label_and_target(predictions[i], labels[i])\n",
        "        subplot = display_one_flower_eval(image, title, subplot, not correct)\n",
        "        if i >= 8:\n",
        "            break;\n",
        "              \n",
        "    plt.tight_layout()\n",
        "    plt.subplots_adjust(wspace=0.1, hspace=0.1)\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "# Model evaluation\n",
        "def plot_metrics(history):\n",
        "    fig, axes = plt.subplots(2, 1, sharex='col', figsize=(30, 15))\n",
        "    axes = axes.flatten()\n",
        "    print(history)\n",
        "    \n",
        "    axes[0].plot(history['loss'], label='Train loss')\n",
        "    axes[0].plot(history['val_loss'], label='Validation loss')\n",
        "    axes[0].legend(loc='best', fontsize=16)\n",
        "    axes[0].set_title('Loss')\n",
        "    axes[0].axvline(np.argmin(history['loss']), linestyle='dashed')\n",
        "    axes[0].axvline(np.argmin(history['val_loss']), linestyle='dashed', color='orange')\n",
        "    \n",
        "    axes[1].plot(history['accuracy'], label='Train accuracy')\n",
        "    axes[1].plot(history['val_accuracy'], label='Validation accuracy')\n",
        "    axes[1].plot([i*1000 for i in y], label='learning rate')\n",
        "    axes[1].legend(loc='best', fontsize=16)\n",
        "    axes[1].set_title('Accuracy')\n",
        "    axes[1].axvline(np.argmax(history['accuracy']), linestyle='dashed')\n",
        "    axes[1].axvline(np.argmax(history['val_accuracy']), linestyle='dashed', color='orange')\n",
        "\n",
        "    plt.xlabel('Epochs', fontsize=22)\n",
        "    sns.despine()\n",
        "    plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2lB2hfc6vUuk"
      },
      "source": [
        "# Training data samples (with augmentation)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "_kg_hide-input": true,
        "trusted": true,
        "scrolled": true,
        "id": "vdQQI7vvvUul"
      },
      "source": [
        "# train_dataset = get_dataset(FILENAMES_COMP, ordered=True, augment=True)\n",
        "# train_iter = iter(train_dataset.unbatch().batch(20))\n",
        "\n",
        "# display_batch_of_images(next(train_iter))\n",
        "# display_batch_of_images(next(train_iter))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IkbU1T33vUum"
      },
      "source": [
        "## Datasets distribution\n",
        "\n",
        "### Competition data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "_kg_hide-input": true,
        "trusted": true,
        "scrolled": true,
        "id": "wgoNPaGEvUun"
      },
      "source": [
        "# ds_comp = get_dataset(FILENAMES_COMP)\n",
        "# labels_comp = [target.numpy() for img, target in iter(ds_comp.unbatch())]\n",
        "# labels_comp = np.argmax(labels_comp, axis=-1)\n",
        "\n",
        "# fig, ax = plt.subplots(1, 1, figsize=(18, 8))\n",
        "# ax = sns.countplot(y=labels_comp, palette='viridis')\n",
        "# ax.tick_params(labelsize=16)\n",
        "\n",
        "# plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jPMHcvEjvUun"
      },
      "source": [
        "### 2019 competition data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "_kg_hide-input": true,
        "trusted": true,
        "scrolled": true,
        "id": "guHn80n4vUuo"
      },
      "source": [
        "# ds_2019 = get_dataset(FILENAMES_2019)\n",
        "# labels_2019 = [target.numpy() for img, target in iter(ds_2019.unbatch())]\n",
        "# labels_2019 = np.argmax(labels_2019, axis=-1)\n",
        "\n",
        "# fig, ax = plt.subplots(1, 1, figsize=(18, 8))\n",
        "# ax = sns.countplot(y=labels_2019, palette='viridis')\n",
        "# ax.tick_params(labelsize=16)\n",
        "\n",
        "# plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lKvzD-dsvUuo"
      },
      "source": [
        "### Dataset oversampled"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "_kg_hide-input": true,
        "trusted": true,
        "scrolled": true,
        "id": "gX4oT07QvUup"
      },
      "source": [
        "# FILENAMES_COMP_OVER = (FILENAMES_COMP + \n",
        "#                        FILENAMES_2019 + \n",
        "#                        (2 * FILENAMES_COMP_CBB) + \n",
        "#                        (2 * FILENAMES_2019_CBB) + \n",
        "#                        (2 * FILENAMES_COMP_CBSD) + \n",
        "#                        (2 * FILENAMES_2019_CBSD) + \n",
        "#                        (2 * FILENAMES_COMP_CGM) + \n",
        "#                        (2 * FILENAMES_2019_CGM) + \n",
        "#                        (2 * FILENAMES_COMP_Healthy) + \n",
        "#                        (2 * FILENAMES_2019_Healthy))\n",
        "\n",
        "# ds_comp = get_dataset(FILENAMES_COMP_OVER)\n",
        "# labels_comp = [target.numpy() for img, target in iter(ds_comp.unbatch())]\n",
        "# labels_comp = np.argmax(labels_comp, axis=-1)\n",
        "\n",
        "# fig, ax = plt.subplots(1, 1, figsize=(18, 8))\n",
        "# ax = sns.countplot(y=labels_comp, palette='viridis')\n",
        "# ax.tick_params(labelsize=16)\n",
        "\n",
        "# plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "papermill": {
          "duration": 0.151388,
          "end_time": "2020-10-10T21:30:00.486807",
          "exception": false,
          "start_time": "2020-10-10T21:30:00.335419",
          "status": "completed"
        },
        "tags": [],
        "id": "pEUuwqL8vUup"
      },
      "source": [
        "### Learning rate schedule\n",
        "\n",
        "We are going to use a `cosine learning rate schedule with a warm-up phase`, this may be a good idea since we are using a pre-trained model, the warm-up phase will be useful to avoid the pre-trained weights degradation resulting in catastrophic forgetting, during the schedule the learning rate will slowly decrease to very low values, this helps the model to land on more stable weights."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "papermill": {
          "duration": 0.149079,
          "end_time": "2020-10-10T21:26:06.196269",
          "exception": false,
          "start_time": "2020-10-10T21:26:06.04719",
          "status": "completed"
        },
        "tags": [],
        "id": "GnAR4KFLvUup"
      },
      "source": [
        "# Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2020-10-10T21:26:06.501284Z",
          "iopub.status.busy": "2020-10-10T21:26:06.500306Z",
          "iopub.status.idle": "2020-10-10T21:26:06.503182Z",
          "shell.execute_reply": "2020-10-10T21:26:06.502614Z"
        },
        "papermill": {
          "duration": 0.159056,
          "end_time": "2020-10-10T21:26:06.50331",
          "exception": false,
          "start_time": "2020-10-10T21:26:06.344254",
          "status": "completed"
        },
        "tags": [],
        "trusted": true,
        "scrolled": true,
        "id": "cQXvVOJsvUuq"
      },
      "source": [
        "def model_fn(input_shape, N_CLASSES):\n",
        "    inputs = L.Input(shape=input_shape, name='input_image')\n",
        "    base_model = cfg.model(input_tensor=inputs, \n",
        "                                    include_top=False, \n",
        "                                    weights='noisy-student', \n",
        "                                    pooling='avg')\n",
        "    base_model.trainable = False\n",
        "    print(f'model is {cfg.model.__name__} and name is {cfg.model_name}')\n",
        "    #x = L.Dropout(.5)(base_model.output)\n",
        "    #output = L.Dense(N_CLASSES, activation='softmax', name='output')(x)\n",
        "    output = L.Dense(N_CLASSES, activation='softmax', name='output')(base_model.output)\n",
        "    model = Model(inputs=inputs, outputs=output)\n",
        "    \n",
        "    unfreeze_model(model)\n",
        "    \n",
        "    if cfg.optimiser_name == 'adam':\n",
        "          optimizer = tf.keras.optimizers.Adam()\n",
        "    elif cfg.optimiser_name== 'ranger':\n",
        "          opt = tfa.optimizers.RectifiedAdam()\n",
        "          optimizer = tfa.optimizers.Lookahead(opt, sync_period=6, slow_step_size=0.5)\n",
        "    else :\n",
        "          optimizer = AdaBeliefOptimizer(learning_rate=1e-3, epsilon=epsilon, rectify=True)\n",
        "    \n",
        "    model.compile(optimizer=optimizer,\n",
        "                      loss=BiTemperedLogisticLoss(t1=T_1, t2=T_2, lbl_smth=SMOOTH_FRACTION, n_iter=N_ITER),\n",
        "                      metrics=['accuracy'], )\n",
        "    return model\n",
        "    \n",
        "\n",
        "    "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VxJdVC5kHZxK"
      },
      "source": [
        "#loss and checkpoint implementation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "scrolled": true,
        "id": "lXCWctldvUuu"
      },
      "source": [
        "#@title Loss implementation\n",
        "\"\"\"Robust Bi-Tempered Logistic Loss Based on Bregman Divergences.\n",
        " Source: https://bit.ly/3jSol8T\n",
        " \"\"\"\n",
        "\n",
        "import functools\n",
        "import tensorflow as tf\n",
        "\n",
        "def for_loop(num_iters, body, initial_args):\n",
        "    \"\"\"Runs a simple for-loop with given body and initial_args.\n",
        "    Args:\n",
        "      num_iters: Maximum number of iterations.\n",
        "      body: Body of the for-loop.\n",
        "      initial_args: Args to the body for the first iteration.\n",
        "    Returns:\n",
        "      Output of the final iteration.\n",
        "    \"\"\"\n",
        "    for i in range(num_iters):\n",
        "        if i == 0:\n",
        "            outputs = body(*initial_args)\n",
        "        else:\n",
        "            outputs = body(*outputs)\n",
        "    return outputs\n",
        "\n",
        "\n",
        "def log_t(u, t):\n",
        "    \"\"\"Compute log_t for `u`.\"\"\"\n",
        "\n",
        "    def _internal_log_t(u, t):\n",
        "        return (u ** (1.0 - t) - 1.0) / (1.0 - t)\n",
        "\n",
        "    return tf.cond(\n",
        "        tf.math.equal(t, 1.0), lambda: tf.math.log(u),\n",
        "        functools.partial(_internal_log_t, u, t))\n",
        "\n",
        "\n",
        "def exp_t(u, t):\n",
        "    \"\"\"Compute exp_t for `u`.\"\"\"\n",
        "\n",
        "    def _internal_exp_t(u, t):\n",
        "        return tf.nn.relu(1.0 + (1.0 - t) * u) ** (1.0 / (1.0 - t))\n",
        "\n",
        "    return tf.cond(\n",
        "        tf.math.equal(t, 1.0), lambda: tf.math.exp(u),\n",
        "        functools.partial(_internal_exp_t, u, t))\n",
        "\n",
        "\n",
        "def compute_normalization_fixed_point(activations, t, num_iters=5):\n",
        "    \"\"\"Returns the normalization value for each example (t > 1.0).\n",
        "    Args:\n",
        "      activations: A multi-dimensional tensor with last dimension `num_classes`.\n",
        "      t: Temperature 2 (> 1.0 for tail heaviness).\n",
        "      num_iters: Number of iterations to run the method.\n",
        "    Return: A tensor of same rank as activation with the last dimension being 1.\n",
        "    \"\"\"\n",
        "\n",
        "    mu = tf.math.reduce_max(activations, -1, keepdims=True)\n",
        "    normalized_activations_step_0 = activations - mu\n",
        "    shape_normalized_activations = tf.shape(normalized_activations_step_0)\n",
        "\n",
        "    def iter_body(i, normalized_activations):\n",
        "        logt_partition = tf.math.reduce_sum(\n",
        "            exp_t(normalized_activations, t), -1, keepdims=True)\n",
        "        normalized_activations_t = tf.reshape(\n",
        "            normalized_activations_step_0 * tf.math.pow(logt_partition, 1.0 - t),\n",
        "            shape_normalized_activations)\n",
        "        return [i + 1, normalized_activations_t]\n",
        "\n",
        "    _, normalized_activations_t = for_loop(num_iters, iter_body,\n",
        "                                           [0, normalized_activations_step_0])\n",
        "\n",
        "    logt_partition = tf.math.reduce_sum(\n",
        "        exp_t(normalized_activations_t, t), -1, keepdims=True)\n",
        "    return -log_t(1.0 / logt_partition, t) + mu\n",
        "\n",
        "\n",
        "def compute_normalization_binary_search(activations, t, num_iters=10):\n",
        "    \"\"\"Returns the normalization value for each example (t < 1.0).\n",
        "    Args:\n",
        "      activations: A multi-dimensional tensor with last dimension `num_classes`.\n",
        "      t: Temperature 2 (< 1.0 for finite support).\n",
        "      num_iters: Number of iterations to run the method.\n",
        "    Return: A tensor of same rank as activation with the last dimension being 1.\n",
        "    \"\"\"\n",
        "    mu = tf.math.reduce_max(activations, -1, keepdims=True)\n",
        "    normalized_activations = activations - mu\n",
        "    shape_activations = tf.shape(activations)\n",
        "    effective_dim = tf.cast(\n",
        "        tf.math.reduce_sum(\n",
        "            tf.cast(\n",
        "                tf.greater(normalized_activations, -1.0 / (1.0 - t)), tf.int32),\n",
        "            -1,\n",
        "            keepdims=True), tf.float32)\n",
        "    shape_partition = tf.concat([shape_activations[:-1], [1]], 0)\n",
        "    lower = tf.zeros(shape_partition)\n",
        "    upper = -log_t(1.0 / effective_dim, t) * tf.ones(shape_partition)\n",
        "\n",
        "    def iter_body(i, lower, upper):\n",
        "        logt_partition = (upper + lower) / 2.0\n",
        "        sum_probs = tf.math.reduce_sum(exp_t(\n",
        "            normalized_activations - logt_partition, t), -1, keepdims=True)\n",
        "        update = tf.cast(tf.less(sum_probs, 1.0), tf.float32)\n",
        "        lower = tf.reshape(lower * update + (1.0 - update) * logt_partition,\n",
        "                           shape_partition)\n",
        "        upper = tf.reshape(upper * (1.0 - update) + update * logt_partition,\n",
        "                           shape_partition)\n",
        "        return [i + 1, lower, upper]\n",
        "\n",
        "    _, lower, upper = for_loop(num_iters, iter_body, [0, lower, upper])\n",
        "    logt_partition = (upper + lower) / 2.0\n",
        "    return logt_partition + mu\n",
        "\n",
        "\n",
        "def compute_normalization(activations, t, num_iters=5):\n",
        "    \"\"\"Returns the normalization value for each example.\n",
        "    Args:\n",
        "      activations: A multi-dimensional tensor with last dimension `num_classes`.\n",
        "      t: Temperature 2 (< 1.0 for finite support, > 1.0 for tail heaviness).\n",
        "      num_iters: Number of iterations to run the method.\n",
        "    Return: A tensor of same rank as activation with the last dimension being 1.\n",
        "    \"\"\"\n",
        "    return tf.cond(\n",
        "        tf.less(t, 1.0),\n",
        "        functools.partial(compute_normalization_binary_search, activations, t,\n",
        "                          num_iters),\n",
        "        functools.partial(compute_normalization_fixed_point, activations, t,\n",
        "                          num_iters))\n",
        "\n",
        "\n",
        "def tempered_softmax(activations, t, num_iters=5):\n",
        "    \"\"\"Tempered softmax function.\n",
        "    Args:\n",
        "      activations: A multi-dimensional tensor with last dimension `num_classes`.\n",
        "      t: Temperature tensor > 0.0.\n",
        "      num_iters: Number of iterations to run the method.\n",
        "    Returns:\n",
        "      A probabilities tensor.\n",
        "    \"\"\"\n",
        "    t = tf.convert_to_tensor(t)\n",
        "    normalization_constants = tf.cond(\n",
        "        tf.math.equal(t, 1.0),\n",
        "        lambda: tf.math.log(tf.math.reduce_sum(tf.exp(activations), -1, keepdims=True)),\n",
        "        functools.partial(compute_normalization, activations, t, num_iters))\n",
        "    return exp_t(activations - normalization_constants, t)\n",
        "\n",
        "\n",
        "def bi_tempered_logistic_loss(activations,\n",
        "                              labels,\n",
        "                              t1,\n",
        "                              t2,\n",
        "                              label_smoothing=0.0,\n",
        "                              num_iters=5):\n",
        "    \"\"\"Bi-Tempered Logistic Loss with custom gradient.\n",
        "    Args:\n",
        "      activations: A multi-dimensional tensor with last dimension `num_classes`.\n",
        "      labels: A tensor with shape and dtype as activations.\n",
        "      t1: Temperature 1 (< 1.0 for boundedness).\n",
        "      t2: Temperature 2 (> 1.0 for tail heaviness, < 1.0 for finite support).\n",
        "      label_smoothing: Label smoothing parameter between [0, 1).\n",
        "      num_iters: Number of iterations to run the method.\n",
        "    Returns:\n",
        "      A loss tensor.\n",
        "    \"\"\"\n",
        "    with tf.name_scope('bitempered_logistic'):\n",
        "        t1 = tf.convert_to_tensor(t1)\n",
        "        t2 = tf.convert_to_tensor(t2)\n",
        "        one = tf.convert_to_tensor(1.0)\n",
        "\n",
        "        if label_smoothing > 0.0:\n",
        "            num_classes = tf.cast(tf.shape(labels)[-1], tf.float32)\n",
        "            labels = (\n",
        "                             1 - num_classes /\n",
        "                             (num_classes - 1) * label_smoothing) * labels + label_smoothing / (\n",
        "                             num_classes - 1)\n",
        "\n",
        "        @tf.custom_gradient\n",
        "        def _custom_gradient_bi_tempered_logistic_loss(activations):\n",
        "            \"\"\"Bi-Tempered Logistic Loss with custom gradient.\n",
        "            Args:\n",
        "              activations: A multi-dimensional tensor with last dim `num_classes`.\n",
        "            Returns:\n",
        "              A loss tensor, grad.\n",
        "            \"\"\"\n",
        "            with tf.name_scope('gradient_bitempered_logistic'):\n",
        "                probabilities = tempered_softmax(activations, t2, num_iters)\n",
        "                loss_values = tf.math.multiply(\n",
        "                    labels,\n",
        "                    log_t(labels + 1e-10, t1) -\n",
        "                    log_t(probabilities, t1)) - 1.0 / (2.0 - t1) * (\n",
        "                                      tf.math.pow(labels, 2.0 - t1) - tf.math.pow(probabilities, 2.0 - t1))\n",
        "\n",
        "                def grad(d_loss):\n",
        "                    \"\"\"Explicit gradient calculation.\n",
        "                    Args:\n",
        "                      d_loss: Infinitesimal change in the loss value.\n",
        "                    Returns:\n",
        "                      Loss gradient.\n",
        "                    \"\"\"\n",
        "                    delta_probs = probabilities - labels\n",
        "                    forget_factor = tf.math.pow(probabilities, t2 - t1)\n",
        "                    delta_probs_times_forget_factor = tf.math.multiply(delta_probs,\n",
        "                                                                       forget_factor)\n",
        "                    delta_forget_sum = tf.math.reduce_sum(\n",
        "                        delta_probs_times_forget_factor, -1, keepdims=True)\n",
        "                    escorts = tf.math.pow(probabilities, t2)\n",
        "                    escorts = escorts / tf.math.reduce_sum(escorts, -1, keepdims=True)\n",
        "                    derivative = delta_probs_times_forget_factor - tf.math.multiply(\n",
        "                        escorts, delta_forget_sum)\n",
        "                    return tf.math.multiply(d_loss, derivative)\n",
        "\n",
        "                return loss_values, grad\n",
        "\n",
        "        loss_values = _custom_gradient_bi_tempered_logistic_loss(activations)\n",
        "\n",
        "        loss_values = tf.math.reduce_sum(loss_values, -1)\n",
        "\n",
        "        return loss_values\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "scrolled": true,
        "id": "bQyvmn0xvUuv"
      },
      "source": [
        "with strategy.scope():\n",
        "  class BiTemperedLogisticLoss(tf.keras.losses.Loss):\n",
        "    def __init__(self, t1, t2, lbl_smth, n_iter):\n",
        "      super(BiTemperedLogisticLoss, self).__init__()\n",
        "      self.t1 = t1\n",
        "      self.t2 = t2\n",
        "      self.lbl_smth = lbl_smth\n",
        "      self.n_iter = n_iter\n",
        "\n",
        "    def call(self, y_true, y_pred):\n",
        "      return bi_tempered_logistic_loss(y_pred, y_true, self.t1, self.t2, self.lbl_smth, self.n_iter)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "scrolled": true,
        "id": "VZDGhnqyvUuv"
      },
      "source": [
        "def get_checkpoint(model_save_path, is_save_best = True):\n",
        "    return ModelCheckpoint(model_save_path, \n",
        "                             monitor= 'val_loss', \n",
        "                             verbose=1, \n",
        "                             save_best_only=is_save_best, \n",
        "                             mode= 'min', \n",
        "                             save_weights_only = False)\n",
        "    \n",
        "def get_early_stopping():\n",
        "    return EarlyStopping(monitor = 'val_loss', min_delta = 0.0001, \n",
        "                           patience = patience, mode = 'min', verbose = 1,\n",
        "                           restore_best_weights = True)\n",
        "    \n",
        "\n",
        "def get_learning_rate_decay():\n",
        "  return ReduceLROnPlateau(monitor = 'val_loss', factor = 0.2, \n",
        "                              patience = 3, min_delta = 0.0001, \n",
        "                              mode = 'min', verbose = 1)\n",
        "\n",
        "\n",
        "def get_model_callback( fold_num):\n",
        "    cfg.model_save_path_best = cfg.model_save_dir+f'{cfg.model_name}_best_fold_{fold_num}_.h5'\n",
        "    print(\"Best model save path: \", cfg.model_save_path_best)\n",
        "\n",
        "    \n",
        "\n",
        "    checkpoint_best = get_checkpoint(cfg.model_save_path_best, is_save_best = True)\n",
        "    #LrPlateau =get_learning_rate_decay()\n",
        "\n",
        "    early_stopping = get_early_stopping()\n",
        "#     learning_rate_decay = get_learning_rate_decay()\n",
        "    lr_callback = tf.keras.callbacks.LearningRateScheduler(lrfn, verbose=True)\n",
        "    \n",
        "#     if WANDB:\n",
        "#         wandb.run.name= f'{MODEL_NAME}_fold_{fold_num}'\n",
        "#         [WandbCallback(),checkpoint_best, checkpoint_last, early_stopping]\n",
        "\n",
        "    return [checkpoint_best, early_stopping , lr_callback]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "papermill": {
          "duration": 0.152125,
          "end_time": "2020-10-10T21:30:01.481735",
          "exception": false,
          "start_time": "2020-10-10T21:30:01.32961",
          "status": "completed"
        },
        "tags": [],
        "id": "R7GdHIOPvUuw"
      },
      "source": [
        "# Training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "scrolled": true,
        "id": "jwaUMmavvUuw"
      },
      "source": [
        "def get_train_vald_ds(idxT,idxV):\n",
        "        FILENAMES_COMP = tf.io.gfile.glob([GCS_PATH + '/Id_train%.2i*.tfrec' % x for x in idxT])      \n",
        "        FILENAMES_COMP_CBB = tf.io.gfile.glob([GCS_PATH_CLASSES + '/CBB%.2i*.tfrec' % x for x in idxT])\n",
        "        FILENAMES_COMP_CBSD = tf.io.gfile.glob([GCS_PATH_CLASSES + '/CBSD%.2i*.tfrec' % x for x in idxT])\n",
        "        FILENAMES_COMP_CGM = tf.io.gfile.glob([GCS_PATH_CLASSES + '/CGM%.2i*.tfrec' % x for x in idxT])\n",
        "        FILENAMES_COMP_Healthy = tf.io.gfile.glob([GCS_PATH_CLASSES + '/Healthy%.2i*.tfrec' % x for x in idxT])\n",
        "        \n",
        "        \n",
        "        if cfg.external_data:\n",
        "            FILENAMES_2019 = tf.io.gfile.glob([GCS_PATH_EXT + '/Id_train%.2i*.tfrec' % x for x in idxT])\n",
        "            FILENAMES_2019_CBB = tf.io.gfile.glob([GCS_PATH_EXT_CLASSES + '/CBB%.2i*.tfrec' % x for x in idxT])\n",
        "            FILENAMES_2019_CBSD = tf.io.gfile.glob([GCS_PATH_EXT_CLASSES + '/CBSD%.2i*.tfrec' % x for x in idxT])\n",
        "            FILENAMES_2019_CGM = tf.io.gfile.glob([GCS_PATH_EXT_CLASSES + '/CGM%.2i*.tfrec' % x for x in idxT])\n",
        "            FILENAMES_2019_Healthy = tf.io.gfile.glob([GCS_PATH_EXT_CLASSES + '/Healthy%.2i*.tfrec' % x for x in idxT])\n",
        "            TRAIN_FILENAMES = (FILENAMES_COMP + \n",
        "                           FILENAMES_2019 + \n",
        "                           (2 * FILENAMES_COMP_CBB) + \n",
        "                           (2 * FILENAMES_2019_CBB) + \n",
        "                           (2 * FILENAMES_COMP_CBSD) + \n",
        "                           (2 * FILENAMES_2019_CBSD) + \n",
        "                           (2 * FILENAMES_COMP_CGM) + \n",
        "                           (2 * FILENAMES_2019_CGM) + \n",
        "                           (2 * FILENAMES_COMP_Healthy) + \n",
        "                           (2 * FILENAMES_2019_Healthy))\n",
        "        else:\n",
        "            TRAIN_FILENAMES = (FILENAMES_COMP + \n",
        "                           \n",
        "                           (2 * FILENAMES_COMP_CBB) + \n",
        "                           (2 * FILENAMES_COMP_CBSD) + \n",
        "                           (2 * FILENAMES_COMP_CGM) + \n",
        "                           (2 * FILENAMES_COMP_Healthy) )\n",
        "                          \n",
        "        \n",
        "\n",
        "        VALID_FILENAMES = tf.io.gfile.glob([GCS_PATH + '/Id_train%.2i*.tfrec' % x for x in idxV])\n",
        "        #np.random.shuffle(TRAIN_FILENAMES)\n",
        "        return TRAIN_FILENAMES,VALID_FILENAMES\n",
        "        \n",
        "    \n",
        "    "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "_kg_hide-input": false,
        "_kg_hide-output": true,
        "trusted": true,
        "scrolled": true,
        "id": "bNT3SLkbvUuw"
      },
      "source": [
        "def train_kfold(exp_num):\n",
        "    \n",
        "    skf = KFold(n_splits=N_FOLDS, shuffle=True, random_state=seed)\n",
        "    oof_pred = []; oof_labels = []; history_list = [];\n",
        "\n",
        "\n",
        "    for fold,(idxT, idxV) in enumerate(skf.split(np.arange(50))):\n",
        "        if fold >= FOLDS_USED:\n",
        "            break\n",
        "        if tpu: tf.tpu.experimental.initialize_tpu_system(tpu)\n",
        "        K.clear_session()\n",
        "        print(f'\\nFOLD: {fold+1}')\n",
        "        #print(f'TRAIN: {idxT} VALID: {idxV}')\n",
        "        \n",
        "        TRAIN_FILENAMES,VALID_FILENAMES = get_train_vald_ds(idxT,idxV)\n",
        "        \n",
        "    \n",
        "        ct_train = count_data_items(TRAIN_FILENAMES)\n",
        "        ct_valid = count_data_items(VALID_FILENAMES)\n",
        "        print(f'count  {ct_train} and {ct_valid}')\n",
        "\n",
        "        step_size = (ct_train / BATCH_SIZE)\n",
        "        valid_step_size = (ct_valid / BATCH_SIZE) \n",
        "        #total_steps=(total_epochs * step_size)\n",
        "        #warmup_steps=(warmup_epochs * step_size)\n",
        "\n",
        "\n",
        "        # Build TF datasets\n",
        "        train_ds =get_dataset(TRAIN_FILENAMES, labeled=True, ordered=False, repeated=True, augment=True)\n",
        "\n",
        "        valid_ds = get_dataset(VALID_FILENAMES, labeled=True,cached=True, ordered=True, repeated=True, augment=False)\n",
        "\n",
        "        callback_list=get_model_callback(fold+1)\n",
        "\n",
        "\n",
        "\n",
        "        with strategy.scope():\n",
        "            model = model_fn((None, None, CHANNELS), N_CLASSES)\n",
        "             # unfreeze all layers except \"batch normalization\"\n",
        "            history = model.fit( x=train_ds,\n",
        "                                    steps_per_epoch = step_size,\n",
        "                                    epochs = EPOCHS,\n",
        "                                    validation_data =  valid_ds,\n",
        "                                    validation_steps = valid_step_size,\n",
        "                                    callbacks = callback_list\n",
        "                                  )\n",
        "\n",
        "        history_list.append(history.history)\n",
        "        oof_val_acc = np.max(history.history['val_accuracy'])\n",
        "        print(f'\\nFOLD: {fold+1}')\n",
        "        plot_metrics(history.history)\n",
        "\n",
        "\n",
        "\n",
        "        ### RESULTS\n",
        "        print(f\"#### FOLD {fold+1} OOF Accuracy = {oof_val_acc:.3f}\")\n",
        "\n",
        "        #history_list.append(history)\n",
        "        # Load best model weights\n",
        "        print(f'loading model from {cfg.model_save_path_best}')\n",
        "        model.load_weights(cfg.model_save_path_best)\n",
        "\n",
        "        # OOF predictions\n",
        "        ds_valid = get_dataset(VALID_FILENAMES, ordered=True)\n",
        "        targets_fold =[target.numpy() for img, target in iter(ds_valid.unbatch())]\n",
        "        oof_labels.append(targets_fold)\n",
        "        x_oof = ds_valid.map(lambda image, target: image)\n",
        "        #oof_pred.append(np.argmax(model.predict(x_oof), axis=-1))\n",
        "        pred_fold = model.predict(x_oof)\n",
        "        oof_pred.append(pred_fold)\n",
        "        auc_score = roc_auc_score(targets_fold, pred_fold, multi_class='ovr')\n",
        "        print(f' roc score for fold {fold} is {auc_score:.3f}')\n",
        "        cfg.df_acc.loc[len(cfg.df_acc)] = { \"Model\": cfg.model_name,\n",
        "                       'Reshape': HEIGHT_RS,\n",
        "                      'accuracy' :f'{oof_val_acc:.4f}',\n",
        "                      'auc' : auc_score,\n",
        "                       'fold' :fold+1,\n",
        "                       'optimizer':cfg.optimiser_name,\n",
        "                       'time':pd.Timestamp.now().strftime(\"%m-%d-%Y %H:%M:%S\")\n",
        "                      }\n",
        "        cfg.df_acc.to_csv(cfg.exp_csv_path,index=False)\n",
        "    \n",
        "    y_true_hot= np.concatenate(oof_labels)\n",
        "    y_true = np.argmax(y_true_hot, axis=-1)\n",
        "    y_pred_hot = np.concatenate(oof_pred)\n",
        "    y_pred = np.argmax(y_pred_hot, axis=-1)\n",
        "    df_oof_hot = pd.DataFrame({'Target':y_true})\n",
        "    y_pred_hot_with_target = np.hstack((y_pred_hot,y_true.reshape(-1,1)))\n",
        "    \n",
        "    \n",
        "    df_oof_hot = pd.DataFrame(y_pred_hot_with_target,columns=list('12345')+['Target'])\n",
        "    df_oof = pd.DataFrame([y_pred,y_true]).T\n",
        "    df_oof.columns=['Pred','Target']\n",
        "    \n",
        "    df_oof.to_csv(Path(cfg.model_save_dir).joinpath(f'{cfg.model_name}_OOF.csv'),index=False)\n",
        "    df_oof_hot.to_csv(Path(cfg.model_save_dir).joinpath(f'{cfg.model_name}_OOF_Hot.csv'),index=False)\n",
        "    \n",
        "    \n",
        "    print(classification_report(y_true, y_pred, target_names=CLASSES))\n",
        "    \n",
        "        \n",
        "    # for fold, history in enumerate(history_list):\n",
        "    #     print(f'\\nFOLD: {fold+1}')\n",
        "    #     plot_metrics(history)\n",
        "    print(cfg.df_acc.tail(10))\n",
        "    print(f'Overall accuracy of the model {accuracy_score(y_true, y_pred)}')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1a1dtmhxH8gL"
      },
      "source": [
        "#LR and runner"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "_kg_hide-input": true,
        "execution": {
          "iopub.execute_input": "2020-10-10T22:09:57.231883Z",
          "iopub.status.busy": "2020-10-10T22:09:57.231124Z",
          "iopub.status.idle": "2020-10-10T22:09:57.902666Z",
          "shell.execute_reply": "2020-10-10T22:09:57.901809Z"
        },
        "papermill": {
          "duration": 0.861991,
          "end_time": "2020-10-10T22:09:57.902823",
          "exception": false,
          "start_time": "2020-10-10T22:09:57.040832",
          "status": "completed"
        },
        "tags": [],
        "trusted": true,
        "scrolled": false,
        "id": "8CReP8KRvUux"
      },
      "source": [
        "\n",
        "\n",
        "\n",
        "if Path(cfg.exp_csv_path).exists():\n",
        "  cfg.df_acc = pd.read_csv(cfg.exp_csv_path)\n",
        "else:\n",
        "  cfg.df_acc = pd.DataFrame(columns=['Model','Reshape','accuracy','auc','optimizer','fold','time'])\n",
        "\n",
        "if cfg.run_exp:\n",
        "  cfg.num_exp=1\n",
        "\n",
        "for i in range(cfg.num_exp):\n",
        "  if not cfg.run_exp:      \n",
        "    cfg.model = cfg.models[i]\n",
        "    cfg.optimiser_name = cfg.optimizers[i]\n",
        "    #cfg.img_resize_shape = cfg.model_input_shape[i]\n",
        "  #print(f\"img reshape size=({cfg.img_resize_shape})\")\n",
        "#   cfg.HEIGHT = cfg.img_resize_shape\n",
        "#   cfg.WIDTH  = cfg.img_resize_shape\n",
        "  cfg.folder_name=str(cfg.colab_instance)+\"__\"+cfg.model_name +\"/\"\n",
        "  cfg.model_save_dir = cfg.base_save_path + cfg.folder_name\n",
        "  train_kfold(i)\n",
        " \n",
        "#   if i<=ds_extreme_index:\n",
        "#     dataset_path = dataset_gs[512]\n",
        "#     print('taking 512 size ds')\n",
        "#   else:\n",
        "#     dataset_path = dataset_gs[512]\n",
        "#     print('taking 512 size ds')\n",
        "#   DATASET_FILENAMES = tf.io.gfile.glob(dataset_path + '/*.tfrec')  \n",
        "#   if display_dataset:\n",
        "#     NUM_FILES = len(DATASET_FILENAMES)\n",
        "#     print(\"Number of files: \",NUM_FILES)\n",
        "#     display(DATASET_FILENAMES)\n",
        "#     display_dataset=False \n",
        "\n",
        "  \n",
        "    \n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}